{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c7fb-a104-4c2f-9f8b-eae3fc40b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\"\"\"___________     __         .__      __________                                .___       ___________       __              ___ ___                        \n",
    "\\_   _____/____/  |_  ____ |  |__   \\______   \\ ______  _  _______ _______  __| _/______ \\__    ___/____  |  | __ ____    /   |   \\  ____   _____   ____  \n",
    " |    __)/ __ \\   __\\/ ___\\|  |  \\   |       _// __ \\ \\/ \\/ /\\__  \\\\_  __ \\/ __ |/  ___/   |    |  \\__  \\ |  |/ // __ \\  /    ~    \\/  _ \\ /     \\_/ __ \\ \n",
    " |     \\\\  ___/|  | \\  \\___|   Y  \\  |    |   \\  ___/\\     /  / __ \\|  | \\/ /_/ |\\___ \\    |    |   / __ \\|    <\\  ___/  \\    Y    (  <_> )  Y Y  \\  ___/ \n",
    " \\___  / \\___  >__|  \\___  >___|  /  |____|_  /\\___  >\\/\\_/  (____  /__|  \\____ /____  >   |____|  (____  /__|_ \\\\___  >  \\___|_  / \\____/|__|_|  /\\___  >\n",
    "     \\/      \\/          \\/     \\/          \\/     \\/             \\/           \\/    \\/                 \\/     \\/    \\/         \\/              \\/     \\/ \"\"\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725c5bb-f859-42c9-8b29-714f4d8a3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Packages\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e720878-92ab-4f07-b7c4-6b6ccd2a1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import json\n",
    "\n",
    "gz_folder = 'source_files'\n",
    "unzipped_data = {}\n",
    "\n",
    "# Loop through all .gz files in the folder and unzip them into a dictionary that holds each respective file \n",
    "for filename in os.listdir(gz_folder):\n",
    "    #Constructing the path to open the file\n",
    "    gz_path = os.path.join(gz_folder, filename)\n",
    "    #Opening the file with rt to convert automatically to text \n",
    "    with gzip.open(gz_path, 'rt') as file:\n",
    "        base_name = filename[:-3]\n",
    "        unzipped_data[base_name] = file.read()\n",
    "\n",
    "\n",
    "print(\"Files extracted:\", list(unzipped_data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f4338-2e28-4624-b592-f0f7c2ad7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looking at Unzipped Data: \", unzipped_data['brands.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e9e1b-b799-4bb6-813a-1fe00ec68e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new dicitionary with JSON parsed as values and filenames as keys\n",
    "pretty_printed_data = {}\n",
    "for filename, file_content in unzipped_data.items():\n",
    "    pretty_objects = []\n",
    "    lines = file_content.strip().split('\\n')\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            pretty_str = json.dumps(json_obj, indent=4)\n",
    "            pretty_objects.append(pretty_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping line {i+1} in {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e1436-3f00-4529-9903-cb14f441eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got some errors in the users.json file. Could be whitespace issues checking to see\n",
    "users_lines = unzipped_data['users.json'].split('\\n')\n",
    "\n",
    "line_numbers_to_check = [0, 495]  # line 1 and line 496\n",
    "\n",
    "for i in line_numbers_to_check:\n",
    "    print(f\"Line {i+1}: {repr(users_lines[i])}\")\n",
    "# The output suggests this is a TAR file probably someone used linux or mac to make it. Next cell down will redo it to account for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "426ef8c2-379d-4bac-b793-74f429820dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted: ['brands.json', 'receipts.json', 'users.json']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas\n",
    "gz_folder = 'source_files'\n",
    "unzipped_data = {}\n",
    "\n",
    "# Loop through all .gz files in the folder and unzip them into a dictionary that holds each respective file \n",
    "for filename in os.listdir(gz_folder):\n",
    "    #Constructing the path to open the file\n",
    "    gz_path = os.path.join(gz_folder, filename)\n",
    "    #Opening the file with rt to convert automatically to text \n",
    "    try:\n",
    "        with tarfile.open(gz_path, 'r:gz') as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile():\n",
    "                    extracted = tar.extractfile(member)\n",
    "                    if extracted:\n",
    "                        content = extracted.read().decode('utf-8')\n",
    "                        lines = content.strip().split('\\n')\n",
    "                        pretty_lines = []\n",
    "                        for line in lines:\n",
    "                            if not line.strip():\n",
    "                                continue\n",
    "                            try:\n",
    "                                json_obj = json.loads(line)\n",
    "                                pretty_lines.append(json.dumps(json_obj, indent=4))\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Skipping line in {member.name}: {e}\")\n",
    "                        unzipped_data[member.name] = '\\n\\n'.join(pretty_lines)\n",
    "    except tarfile.ReadError:\n",
    "        with gzip.open(gz_path, 'rt') as file:\n",
    "            base_name = filename[:-3]\n",
    "# In case TAR unzip doesn't work\n",
    "            unzipped_data[base_name] = file.read()\n",
    "\n",
    "print(\"Files extracted:\", list(unzipped_data.keys()))\n",
    "# print(unzipped_data['brands.json'])\n",
    "# print(unzipped_data['users.json'])\n",
    "# print(unzipped_data['receipts.json'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c7b39f9-783c-4668-b8fd-8b558c8bf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Explicitly define which files use NDJSON structure\n",
    "ndjson_files = {'receipts.json', 'brands.json'}\n",
    "pretty_json_files = {'users.json'}\n",
    "\n",
    "for filename, content in unzipped_data.items():\n",
    "    try:\n",
    "        stripped = content.strip()\n",
    "\n",
    "        if filename in ndjson_files:\n",
    "            # NDJSON: each line is a standalone JSON object\n",
    "            json_objects = [\n",
    "                json.loads(line)\n",
    "                for line in stripped.splitlines()\n",
    "                if line.strip()\n",
    "            ]\n",
    "        elif filename in pretty_json_files:\n",
    "            # Pretty-printed or compact JSON with blank-line separation\n",
    "            blocks = stripped.split('\\n\\n')\n",
    "            json_objects = [\n",
    "                json.loads(block)\n",
    "                for block in blocks\n",
    "                if block.strip()\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format for file: {filename}\")\n",
    "\n",
    "        df = pd.DataFrame(json_objects)\n",
    "        cleaned_dataframes[filename] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {filename} into DataFrame: {e}\")\n",
    "# print(\"Users json: \",cleaned_dataframes['users.json'].head())\n",
    "# print(\"Brands json: \",cleaned_dataframes['brands.json'].head())\n",
    "# print(cleaned_dataframes['brands.json']['cpg'].head())\n",
    "\n",
    "# print(\"Receipts json: \",cleaned_dataframes['receipts.json'].head())\n",
    "# Looks like I need to parse out the dates which were sent as nested objects and the ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1474e584-d8e3-4cdf-a566-c79acef9494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users json:                          _id  active             createdDate  \\\n",
      "0  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "1  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "2  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "3  5ff1e1eacfcf6c399c274ae6    True 2021-01-03 09:25:30.554   \n",
      "4  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "\n",
      "                lastLogin      role signUpSource state  \n",
      "0 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "1 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "2 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "3 2021-01-03 09:25:30.597  consumer        Email    WI  \n",
      "4 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "Brands json:                          _id       barcode        category      categoryCode  \\\n",
      "0  601ac115be37ce2ead437551  511111019862          Baking            BAKING   \n",
      "1  601c5460be37ce2ead43755f  511111519928       Beverages         BEVERAGES   \n",
      "2  601ac142be37ce2ead43755d  511111819905          Baking            BAKING   \n",
      "3  601ac142be37ce2ead43755a  511111519874          Baking            BAKING   \n",
      "4  601ac142be37ce2ead43755e  511111319917  Candy & Sweets  CANDY_AND_SWEETS   \n",
      "\n",
      "                                                 cpg  \\\n",
      "0  {'$id': {'$oid': '601ac114be37ce2ead437550'}, ...   \n",
      "1  {'$id': {'$oid': '5332f5fbe4b03c9a25efd0ba'}, ...   \n",
      "2  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "3  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "4  {'$id': {'$oid': '5332fa12e4b03c9a25efd1e7'}, ...   \n",
      "\n",
      "                        name topBrand                      brandCode  \n",
      "0  test brand @1612366101024    False                            NaN  \n",
      "1                  Starbucks    False                      STARBUCKS  \n",
      "2  test brand @1612366146176    False  TEST BRANDCODE @1612366146176  \n",
      "3  test brand @1612366146051    False  TEST BRANDCODE @1612366146051  \n",
      "4  test brand @1612366146827    False  TEST BRANDCODE @1612366146827  \n",
      "Receipts json:                          _id  bonusPointsEarned  \\\n",
      "0  5ff1e1eb0a720f0523000575              500.0   \n",
      "1  5ff1e1bb0a720f052300056b              150.0   \n",
      "2  5ff1e1f10a720f052300057a                5.0   \n",
      "3  5ff1e1ee0a7214ada100056f                5.0   \n",
      "4  5ff1e1d20a7214ada1000561                5.0   \n",
      "\n",
      "                             bonusPointsEarnedReason          createDate  \\\n",
      "0  Receipt number 2 completed, bonus point schedu... 2021-01-03 09:25:31   \n",
      "1  Receipt number 5 completed, bonus point schedu... 2021-01-03 09:24:43   \n",
      "2                         All-receipts receipt bonus 2021-01-03 09:25:37   \n",
      "3                         All-receipts receipt bonus 2021-01-03 09:25:34   \n",
      "4                         All-receipts receipt bonus 2021-01-03 09:25:06   \n",
      "\n",
      "          dateScanned        finishedDate          modifyDate  \\\n",
      "0 2021-01-03 09:25:31 2021-01-03 09:25:31 2021-01-03 09:25:36   \n",
      "1 2021-01-03 09:24:43 2021-01-03 09:24:43 2021-01-03 09:24:48   \n",
      "2 2021-01-03 09:25:37                 NaT 2021-01-03 09:25:42   \n",
      "3 2021-01-03 09:25:34 2021-01-03 09:25:34 2021-01-03 09:25:39   \n",
      "4 2021-01-03 09:25:06 2021-01-03 09:25:11 2021-01-03 09:25:11   \n",
      "\n",
      "    pointsAwardedDate pointsEarned        purchaseDate  purchasedItemCount  \\\n",
      "0 2021-01-03 09:25:31        500.0 2021-01-02 18:00:00                 5.0   \n",
      "1 2021-01-03 09:24:43        150.0 2021-01-02 09:24:43                 2.0   \n",
      "2                 NaT            5 2021-01-02 18:00:00                 1.0   \n",
      "3 2021-01-03 09:25:34          5.0 2021-01-02 18:00:00                 4.0   \n",
      "4 2021-01-03 09:25:06          5.0 2021-01-02 09:25:06                 2.0   \n",
      "\n",
      "                              rewardsReceiptItemList rewardsReceiptStatus  \\\n",
      "0  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "1  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "2  [{'needsFetchReview': False, 'partnerItemId': ...             REJECTED   \n",
      "3  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "4  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "\n",
      "  totalSpent                    userId  \n",
      "0      26.00  5ff1e1eacfcf6c399c274ae6  \n",
      "1      11.00  5ff1e194b6a9d73a3a9f1052  \n",
      "2      10.00  5ff1e1f1cfcf6c399c274b0b  \n",
      "3      28.00  5ff1e1eacfcf6c399c274ae6  \n",
      "4       1.00  5ff1e194b6a9d73a3a9f1052  \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Cleaning up id objects and dates. \n",
    "for df in cleaned_dataframes.values():\n",
    "    # 1. Normalize _id.$oid to a string\n",
    "    if '_id' in df.columns:\n",
    "        df['_id'] = df['_id'].apply(\n",
    "            lambda x: x.get('$oid') if isinstance(x, dict) and '$oid' in x else x\n",
    "        )\n",
    "\n",
    "    # 2. Normalize all columns that include \"date\" (case-insensitive)\n",
    "    for col in df.columns:\n",
    "        if 'date' or 'lastlogin' in col.lower():\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: datetime.fromtimestamp(x['$date'] / 1000.0)\n",
    "                if isinstance(x, dict) and '$date' in x else x\n",
    "            )\n",
    "print(\"Users json: \",cleaned_dataframes['users.json'].head())\n",
    "print(\"Brands json: \",cleaned_dataframes['brands.json'].head())\n",
    "print(\"Receipts json: \",cleaned_dataframes['receipts.json'].head())\n",
    "# Looks like I can't just key on date adding lastLogin from Users as well. \n",
    "\n",
    "# is_unique = cleaned_dataframes['brands.json']['brandCode'].is_unique\n",
    "# print(\"Is 'name' column unique in brands.json?:\", is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fafff34e-4ff0-4e49-95e0-04511d65e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users.json columns: ['_id', 'active', 'createdDate', 'lastLogin', 'role', 'signUpSource', 'state']\n",
      "                        _id  active             createdDate  \\\n",
      "0  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "1  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "\n",
      "                lastLogin      role signUpSource state  \n",
      "0 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "1 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "brands.json columns: ['_id', 'barcode', 'category', 'categoryCode', 'cpg', 'name', 'topBrand', 'brandCode']\n",
      "                        _id       barcode   category categoryCode  \\\n",
      "0  601ac115be37ce2ead437551  511111019862     Baking       BAKING   \n",
      "1  601c5460be37ce2ead43755f  511111519928  Beverages    BEVERAGES   \n",
      "\n",
      "                                                 cpg  \\\n",
      "0  {'$id': {'$oid': '601ac114be37ce2ead437550'}, ...   \n",
      "1  {'$id': {'$oid': '5332f5fbe4b03c9a25efd0ba'}, ...   \n",
      "\n",
      "                        name topBrand  brandCode  \n",
      "0  test brand @1612366101024    False        NaN  \n",
      "1                  Starbucks    False  STARBUCKS  \n",
      "receipts.json columns: ['_id', 'bonusPointsEarned', 'bonusPointsEarnedReason', 'createDate', 'dateScanned', 'finishedDate', 'modifyDate', 'pointsAwardedDate', 'pointsEarned', 'purchaseDate', 'purchasedItemCount', 'rewardsReceiptItemList', 'rewardsReceiptStatus', 'totalSpent', 'userId']\n",
      "                        _id  bonusPointsEarned  \\\n",
      "0  5ff1e1eb0a720f0523000575              500.0   \n",
      "1  5ff1e1bb0a720f052300056b              150.0   \n",
      "\n",
      "                             bonusPointsEarnedReason          createDate  \\\n",
      "0  Receipt number 2 completed, bonus point schedu... 2021-01-03 09:25:31   \n",
      "1  Receipt number 5 completed, bonus point schedu... 2021-01-03 09:24:43   \n",
      "\n",
      "          dateScanned        finishedDate          modifyDate  \\\n",
      "0 2021-01-03 09:25:31 2021-01-03 09:25:31 2021-01-03 09:25:36   \n",
      "1 2021-01-03 09:24:43 2021-01-03 09:24:43 2021-01-03 09:24:48   \n",
      "\n",
      "    pointsAwardedDate pointsEarned        purchaseDate  purchasedItemCount  \\\n",
      "0 2021-01-03 09:25:31        500.0 2021-01-02 18:00:00                 5.0   \n",
      "1 2021-01-03 09:24:43        150.0 2021-01-02 09:24:43                 2.0   \n",
      "\n",
      "                              rewardsReceiptItemList rewardsReceiptStatus  \\\n",
      "0  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "1  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "\n",
      "  totalSpent                    userId  \n",
      "0      26.00  5ff1e1eacfcf6c399c274ae6  \n",
      "1      11.00  5ff1e194b6a9d73a3a9f1052  \n"
     ]
    }
   ],
   "source": [
    "# users.json\n",
    "print(\"users.json columns:\", list(cleaned_dataframes['users.json'].columns))\n",
    "print(cleaned_dataframes['users.json'].head(2))\n",
    "\n",
    "# brands.json\n",
    "print(\"brands.json columns:\", list(cleaned_dataframes['brands.json'].columns))\n",
    "print(cleaned_dataframes['brands.json'].head(2))\n",
    "\n",
    "# receipts.json\n",
    "print(\"receipts.json columns:\", list(cleaned_dataframes['receipts.json'].columns))\n",
    "print(cleaned_dataframes['receipts.json'].head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2386938e-57d7-4880-a95f-90500f770bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a database and tables\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create in-memory SQLite DB\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "create_statements = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE dates (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,          -- surrogate key\n",
    "        calendar_date DATE UNIQUE,\n",
    "        year INTEGER,\n",
    "        month INTEGER,\n",
    "        day INTEGER,\n",
    "        day_of_week INTEGER,\n",
    "        week_of_year INTEGER,\n",
    "        is_weekend BOOLEAN\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE roles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        role_name TEXT UNIQUE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE sign_up_sources (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        source_name TEXT UNIQUE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE states (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        state_code TEXT UNIQUE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE users (\n",
    "        id TEXT PRIMARY KEY UNIQUE,\n",
    "        active BOOLEAN,\n",
    "        createdDate DATETIME,\n",
    "        lastLogin DATETIME,\n",
    "        created_date_id INTEGER,\n",
    "        last_login_date_id INTEGER,\n",
    "        role_id INTEGER,\n",
    "        signup_source_id INTEGER,\n",
    "        state_id INTEGER,\n",
    "        FOREIGN KEY (created_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (last_login_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (role_id) REFERENCES roles(id),\n",
    "        FOREIGN KEY (signup_source_id) REFERENCES sign_up_sources(id),\n",
    "        FOREIGN KEY (state_id) REFERENCES states(id)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE categories (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        category TEXT,\n",
    "        category_code TEXT,\n",
    "        UNIQUE(category, category_code)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE cpgs (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        ref TEXT UNIQUE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE brands (\n",
    "        id TEXT PRIMARY KEY UNIQUE,\n",
    "        name TEXT,\n",
    "        barcode TEXT,\n",
    "        top_brand BOOLEAN,\n",
    "        brand_code TEXT,\n",
    "        category_id INTEGER,\n",
    "        cpg_id TEXT,\n",
    "        FOREIGN KEY (category_id) REFERENCES categories(id),\n",
    "        FOREIGN KEY (cpg_id) REFERENCES cpgs(id)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE receipt_statuses (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        status TEXT UNIQUE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE receipts (\n",
    "        id TEXT PRIMARY KEY UNIQUE,\n",
    "        user_id INTEGER,\n",
    "        bonus_points_earned INTEGER,\n",
    "        bonus_points_reason TEXT,\n",
    "        create_date_id INTEGER,\n",
    "        date_scanned_id INTEGER,\n",
    "        finished_date_id INTEGER,\n",
    "        modify_date_id INTEGER,\n",
    "        points_awarded_date_id INTEGER,\n",
    "        purchase_date_id INTEGER,\n",
    "        points_earned REAL,\n",
    "        purchased_item_count INTEGER,\n",
    "        total_spent REAL,\n",
    "        receipt_status_id INTEGER,\n",
    "        FOREIGN KEY (user_id) REFERENCES users(id),\n",
    "        FOREIGN KEY (create_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (date_scanned_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (finished_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (modify_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (points_awarded_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (purchase_date_id) REFERENCES dates(id),\n",
    "        FOREIGN KEY (receipt_status_id) REFERENCES receipt_statuses(id)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE receipt_items (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        receipt_id TEXT,\n",
    "        barcode TEXT,\n",
    "        description TEXT,\n",
    "        final_price REAL,\n",
    "        item_price REAL,\n",
    "        quantity_purchased INTEGER,\n",
    "        user_flagged_new_item BOOLEAN,\n",
    "        user_flagged_barcode TEXT,\n",
    "        user_flagged_price REAL,\n",
    "        user_flagged_quantity INTEGER,\n",
    "        needs_fetch_review BOOLEAN,\n",
    "        partner_item_id TEXT,\n",
    "        points_not_awarded_reason TEXT,\n",
    "        points_payer_id TEXT,\n",
    "        rewards_group TEXT,\n",
    "        rewards_product_partner_id TEXT,\n",
    "        user_flagged_description TEXT,\n",
    "        FOREIGN KEY (receipt_id) REFERENCES receipts(id)\n",
    "        FOREIGN KEY (points_payer_id) REFERENCES users(id)\n",
    "        FOREIGN KEY (rewards_product_partner_id) REFERENCES users(id)\n",
    "    );\n",
    "    \"\"\"]\n",
    "\n",
    "\n",
    "#creating tables\n",
    "for stmt in create_statements:\n",
    "    cur.execute(stmt)\n",
    "\n",
    "\n",
    "# Populate with 20 years of dates\n",
    "start_date = datetime(2005, 1, 1)\n",
    "end_date = datetime(2025, 7, 31)\n",
    "delta = end_date - start_date\n",
    "\n",
    "for i in range(delta.days + 1):\n",
    "    date_val = start_date + timedelta(days=i)\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO dates\n",
    "              (calendar_date, year, month, day,\n",
    "               day_of_week, week_of_year, is_weekend)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        date_val.strftime('%Y-%m-%d'),\n",
    "        date_val.year,\n",
    "        date_val.month,\n",
    "        date_val.day,\n",
    "        date_val.weekday(),\n",
    "        date_val.isocalendar()[1],\n",
    "        int(date_val.weekday() >= 5)\n",
    "    ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e7e058f-1faf-4767-967b-93c30a254de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main ETL script\n",
    "import json, math\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#helper functions to handle nulls etc.\n",
    "def safe_ts(ts):\n",
    "    \"\"\"Return ISO 'YYYY‑MM‑DD HH:MM:SS' or None.\"\"\"\n",
    "    if pd.isna(ts) or ts in (\"\", None):\n",
    "        return None\n",
    "    return pd.to_datetime(ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def safe_dt(ts):\n",
    "    \"\"\"Return a date object or None.\"\"\"\n",
    "    if pd.isna(ts) or ts in (\"\", None):\n",
    "        return None\n",
    "    return pd.to_datetime(ts).date()\n",
    "\n",
    "def safe_int(val, default=None):\n",
    "    \"\"\"Convert to int, handling NaN / None / '' gracefully.\"\"\"\n",
    "    if val in (\"\", None) or (isinstance(val, float) and math.isnan(val)):\n",
    "        return default\n",
    "    try:\n",
    "        return int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_float(val, default=None):\n",
    "    \"\"\"Return float or default (for NaN / None / '').\"\"\"\n",
    "    if val in (\"\", None) or (isinstance(val, float) and math.isnan(val)):\n",
    "        return default\n",
    "    try:\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "# CPGs  – extract $oid\n",
    "def extract_cpg_id(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = json.loads(val)\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(val, dict):\n",
    "        return val.get(\"$oid\") or val.get(\"$id\", {}).get(\"$oid\")\n",
    "    return None\n",
    "def ensure_date(conn, d):\n",
    "    \"\"\"Insert d into dates dimension if not present.\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT 1 FROM dates WHERE calendar_date = ?\", (d,))\n",
    "    if cur.fetchone():\n",
    "        return\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO dates\n",
    "              (calendar_date, year, month, day,\n",
    "               day_of_week, week_of_year, is_weekend)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (d.isoformat(), d.year, d.month, d.day,\n",
    "         d.weekday(), d.isocalendar()[1], int(d.weekday() >= 5))\n",
    "    )\n",
    "    conn.commit()\n",
    "def get_date_id(conn, d):\n",
    "    \"\"\"\n",
    "    Ensure d exists in the dates dimension and return its surrogate id.\n",
    "    Returns None if d is None.\n",
    "    \"\"\"\n",
    "    if d is None:\n",
    "        return None\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id FROM dates WHERE calendar_date = ?\", (d.isoformat(),))\n",
    "    row = cur.fetchone()\n",
    "    if row:\n",
    "        return row[0]\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO dates\n",
    "              (calendar_date, year, month, day,\n",
    "               day_of_week, week_of_year, is_weekend)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (d.isoformat(), d.year, d.month, d.day,\n",
    "         d.weekday(), d.isocalendar()[1], int(d.weekday() >= 5))\n",
    "    )\n",
    "    return cur.lastrowid\n",
    "\n",
    "def fk_id(series, key):\n",
    "    \"\"\"Look up surrogate key → int or None.\"\"\"\n",
    "    if key is None or key == \"\" or (isinstance(key, float) and math.isnan(key)):\n",
    "        return None\n",
    "    val = series.get(key)\n",
    "    return int(val) if val is not None else None\n",
    "\n",
    "#staging data frames\n",
    "users_df    = cleaned_dataframes[\"users.json\"].copy()\n",
    "brands_df   = cleaned_dataframes[\"brands.json\"].copy()\n",
    "receipts_df = cleaned_dataframes[\"receipts.json\"].copy()\n",
    "\n",
    "#converting objects in json to text to comply with inserting it into the table\n",
    "for df in (users_df, brands_df, receipts_df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x\n",
    "            )\n",
    "\n",
    "# DB cursor\n",
    "cur = conn.cursor()\n",
    "#deriving roles from the user table and dropping empty values\n",
    "for role in users_df[\"role\"].dropna().unique():\n",
    "    cur.execute(\"INSERT OR IGNORE INTO roles (role_name) VALUES (?)\", (role,))\n",
    "#deriving sign up sources and dropping nulls\n",
    "for src in users_df[\"signUpSource\"].dropna().unique():\n",
    "    cur.execute(\"INSERT OR IGNORE INTO sign_up_sources (source_name) VALUES (?)\", (src,))\n",
    "#getting all state codes and inserting them instead of deriving\n",
    "US_STATE_CODES = {\n",
    "    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\n",
    "    \"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\n",
    "    \"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\n",
    "    \"WI\",\"WY\",\"DC\",\"PR\",\"VI\",\"GU\",\"MP\",\"AS\"\n",
    "}\n",
    "cur.executemany(\n",
    "    \"INSERT OR IGNORE INTO states (state_code) VALUES (?)\",\n",
    "    [(code,) for code in US_STATE_CODES]\n",
    ")\n",
    "# deriving receipt statuses and dropping nulls\n",
    "for status in receipts_df[\"rewardsReceiptStatus\"].dropna().unique():\n",
    "    cur.execute(\"INSERT OR IGNORE INTO receipt_statuses (status) VALUES (?)\", (status,))\n",
    "\n",
    "# deriving distinct categories\n",
    "cat_pairs = brands_df[[\"category\", \"categoryCode\"]].drop_duplicates()\n",
    "cur.executemany(\n",
    "    \"INSERT OR IGNORE INTO categories (category, category_code) VALUES (?, ?)\",\n",
    "    [(r[\"category\"], r[\"categoryCode\"]) for _, r in cat_pairs.iterrows()]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "brands_df[\"cpg_extracted\"] = brands_df[\"cpg\"].apply(extract_cpg_id)\n",
    "for cpg_id in brands_df[\"cpg_extracted\"].dropna().unique():\n",
    "    cur.execute(\"INSERT OR IGNORE INTO cpgs (id) VALUES (?)\", (cpg_id,))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "# Look‑up Series for FK mapping\n",
    "role_ids   = pd.read_sql(\"SELECT id, role_name FROM roles\", conn).set_index(\"role_name\")[\"id\"]\n",
    "src_ids    = pd.read_sql(\"SELECT id, source_name FROM sign_up_sources\", conn).set_index(\"source_name\")[\"id\"]\n",
    "state_ids  = pd.read_sql(\"SELECT id, state_code FROM states\", conn).set_index(\"state_code\")[\"id\"]\n",
    "status_ids = pd.read_sql(\"SELECT id, status FROM receipt_statuses\", conn).set_index(\"status\")[\"id\"]\n",
    "\n",
    "cat_df = pd.read_sql(\"SELECT id, category, category_code FROM categories\", conn)\n",
    "cat_df[\"key\"] = cat_df[\"category\"] + \"||\" + cat_df[\"category_code\"]\n",
    "cat_lookup = cat_df.set_index(\"key\")[\"id\"]\n",
    "\n",
    "# converting users df into tuples that can be dynamically inserted\n",
    "user_rows = [\n",
    "    (\n",
    "        r[\"_id\"],\n",
    "        safe_int(r[\"active\"], 0),\n",
    "        safe_ts(r[\"createdDate\"]),\n",
    "        safe_ts(r[\"lastLogin\"]),\n",
    "        get_date_id(conn, safe_dt(r[\"createdDate\"])),  \n",
    "        get_date_id(conn, safe_dt(r[\"lastLogin\"])),    \n",
    "        fk_id(role_ids,   r[\"role\"]),\n",
    "        fk_id(src_ids,    r[\"signUpSource\"]),\n",
    "        fk_id(state_ids,  r[\"state\"])\n",
    "    )\n",
    "    for _, r in users_df.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "cur.executemany(\n",
    "    \"\"\"\n",
    "    INSERT OR IGNORE INTO users\n",
    "          (id, active, createdDate, lastLogin,\n",
    "           created_date_id, last_login_date_id,\n",
    "           role_id, signup_source_id, state_id)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "    user_rows\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "#converting brand rows into tuples so they can be inserted\n",
    "brand_rows = [\n",
    "    (\n",
    "        r[\"_id\"],\n",
    "        r[\"name\"],\n",
    "        r[\"barcode\"],\n",
    "        safe_int(r[\"topBrand\"], 0),\n",
    "        r[\"brandCode\"],\n",
    "        fk_id(cat_lookup, f\"{r['category']}||{r['categoryCode']}\"),\n",
    "        r[\"cpg_extracted\"]\n",
    "    )\n",
    "    for _, r in brands_df.iterrows()\n",
    "]\n",
    "\n",
    "cur.executemany(\n",
    "    \"\"\"\n",
    "    INSERT OR IGNORE INTO brands\n",
    "          (id, name, barcode, top_brand, brand_code,\n",
    "           category_id, cpg_id)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "    brand_rows\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# receipts and receipt_items\n",
    "receipts_rows, receipt_items_rows = [], []\n",
    "user_ids_set = set(pd.read_sql(\"SELECT id FROM users\", conn)[\"id\"])\n",
    "\n",
    "for _, r in receipts_df.iterrows():\n",
    "    # skip receipts whose user wasn’t loaded\n",
    "    if r[\"userId\"] not in user_ids_set:\n",
    "        continue\n",
    "\n",
    "    receipts_rows.append(\n",
    "        (\n",
    "            r[\"_id\"],\n",
    "            r[\"userId\"],\n",
    "            safe_int(r.get(\"bonusPointsEarned\"), 0),\n",
    "            r.get(\"bonusPointsEarnedReason\"),\n",
    "            get_date_id(conn, safe_dt(r[\"createDate\"])),        # ← surrogate id\n",
    "            get_date_id(conn, safe_dt(r[\"dateScanned\"])),       # ← surrogate id\n",
    "            get_date_id(conn, safe_dt(r[\"finishedDate\"])),      # ← surrogate id\n",
    "            get_date_id(conn, safe_dt(r[\"modifyDate\"])),        # ← surrogate id\n",
    "            get_date_id(conn, safe_dt(r[\"pointsAwardedDate\"])), # ← surrogate id\n",
    "            get_date_id(conn, safe_dt(r[\"purchaseDate\"])),      # ← surrogate id\n",
    "            safe_float(r.get(\"pointsEarned\")),\n",
    "            safe_int(r.get(\"purchasedItemCount\")),\n",
    "            safe_float(r.get(\"totalSpent\")),\n",
    "            fk_id(status_ids, r[\"rewardsReceiptStatus\"])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    raw_items = r[\"rewardsReceiptItemList\"]\n",
    "    try:\n",
    "        items = raw_items if isinstance(raw_items, list) else json.loads(raw_items or \"[]\")\n",
    "    except Exception:\n",
    "        items = []\n",
    "\n",
    "    for itm in items:\n",
    "        receipt_items_rows.append(\n",
    "            (\n",
    "                r[\"_id\"],\n",
    "                itm.get(\"barcode\"),\n",
    "                itm.get(\"description\"),\n",
    "                safe_float(itm.get(\"finalPrice\")),\n",
    "                safe_float(itm.get(\"itemPrice\")),\n",
    "                safe_int(itm.get(\"quantityPurchased\")),\n",
    "                safe_int(itm.get(\"userFlaggedNewItem\"), 0),\n",
    "                itm.get(\"userFlaggedBarcode\"),\n",
    "                safe_float(itm.get(\"userFlaggedPrice\")),\n",
    "                safe_int(itm.get(\"userFlaggedQuantity\")),\n",
    "                safe_int(itm.get(\"needsFetchReview\"), 0),\n",
    "                itm.get(\"partnerItemId\"),\n",
    "                itm.get(\"pointsNotAwardedReason\"),\n",
    "                itm.get(\"pointsPayerId\"),\n",
    "                itm.get(\"rewardsGroup\"),\n",
    "                itm.get(\"rewardsProductPartnerId\"),\n",
    "                itm.get(\"userFlaggedDescription\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "cur.executemany(\n",
    "    \"\"\"\n",
    "    INSERT OR IGNORE INTO receipts\n",
    "          (id, user_id, bonus_points_earned, bonus_points_reason,\n",
    "           create_date_id, date_scanned_id, finished_date_id,\n",
    "           modify_date_id, points_awarded_date_id, purchase_date_id,\n",
    "           points_earned, purchased_item_count, total_spent,\n",
    "           receipt_status_id)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "    receipts_rows\n",
    ")\n",
    "cur.executemany(\n",
    "    \"\"\"\n",
    "    INSERT INTO receipt_items\n",
    "        (receipt_id, barcode, description, final_price, item_price,\n",
    "         quantity_purchased, user_flagged_new_item, user_flagged_barcode,\n",
    "         user_flagged_price, user_flagged_quantity, needs_fetch_review,\n",
    "         partner_item_id, points_not_awarded_reason, points_payer_id,\n",
    "         rewards_group, rewards_product_partner_id, user_flagged_description)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "    receipt_items_rows\n",
    ")\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89cae68e-861e-4e24-812e-2f8590aa9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id     status\n",
      "0   1   FINISHED\n",
      "1   2   REJECTED\n",
      "2   3    FLAGGED\n",
      "3   4  SUBMITTED\n",
      "4   5    PENDING\n",
      "                           id  active          createdDate  \\\n",
      "0    5ff1e194b6a9d73a3a9f1052       1  2021-01-03 09:24:04   \n",
      "1    5ff1e1eacfcf6c399c274ae6       1  2021-01-03 09:25:30   \n",
      "2    5ff1e1e8cfcf6c399c274ad9       1  2021-01-03 09:25:28   \n",
      "3    5ff1e1b7cfcf6c399c274a5a       1  2021-01-03 09:24:39   \n",
      "4    5ff1e1f1cfcf6c399c274b0b       1  2021-01-03 09:25:37   \n",
      "..                        ...     ...                  ...   \n",
      "207  5fc961c3b8cfca11a077dd33       1  2020-12-03 16:08:03   \n",
      "208  5fa41775898c7a11a6bcef3e       1  2020-11-05 09:17:09   \n",
      "209  5fa32b4d898c7a11a6bcebce       1  2020-11-04 16:29:33   \n",
      "210  5964eb07e4b03efd0c0f267b       1  2017-07-11 10:13:11   \n",
      "211  54943462e4b07e684157a532       1  2014-12-19 08:21:22   \n",
      "\n",
      "               lastLogin  created_date_id  last_login_date_id  role_id  \\\n",
      "0    2021-01-03 09:25:37             5847              5847.0        1   \n",
      "1    2021-01-03 09:25:30             5847              5847.0        1   \n",
      "2    2021-01-03 09:25:28             5847              5847.0        1   \n",
      "3    2021-01-03 09:24:39             5847              5847.0        1   \n",
      "4    2021-01-03 09:25:37             5847              5847.0        1   \n",
      "..                   ...              ...                 ...      ...   \n",
      "207  2021-02-26 16:39:16             5816              5901.0        2   \n",
      "208  2021-03-04 10:02:02             5788              5907.0        2   \n",
      "209  2021-03-04 01:21:58             5787              5907.0        2   \n",
      "210  2021-03-04 13:07:49             4575              5907.0        2   \n",
      "211  2021-03-05 10:52:23             3640              5908.0        2   \n",
      "\n",
      "     signup_source_id  state_id  \n",
      "0                 1.0      43.0  \n",
      "1                 1.0      43.0  \n",
      "2                 1.0      43.0  \n",
      "3                 1.0      43.0  \n",
      "4                 1.0      43.0  \n",
      "..                ...       ...  \n",
      "207               1.0      33.0  \n",
      "208               1.0       NaN  \n",
      "209               2.0      11.0  \n",
      "210               NaN      25.0  \n",
      "211               NaN       NaN  \n",
      "\n",
      "[212 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_receipt_statuses = pd.read_sql_query(\"\"\"\n",
    "    SELECT  *\n",
    "    FROM     receipt_statuses \n",
    "\n",
    "\"\"\", conn)\n",
    "print(df_receipt_statuses)\n",
    "df_quer = pd.read_sql_query(\"\"\"\n",
    "  SELECT *\n",
    "  FROM   users\n",
    "  -- WHERE  createdDate >= date('now', '-6 months')\n",
    "\n",
    "\"\"\", conn)\n",
    "print(df_quer)\n",
    "# looks like the statuses don't match the question prompt.\n",
    "# er diagram: https://www.dbdiagram.io/d/fetch-6824047e5b2fc4582f7cbc41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9955516-773d-4a8e-820e-b61b73111db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.\n",
      "Top 5 Brands from the most recent month\n",
      "              brand_name  receipt_count\n",
      "0               Tostitos             10\n",
      "1                Swanson             10\n",
      "2           Kettle Brand              3\n",
      "3  Cracker Barrel Cheese              2\n",
      "4                 Jell-O              2\n",
      "Question 2.\n",
      "Top 5 Brands most recent month vs. previous month\n",
      "    Rank       MonthType                  Brand\n",
      "0      1    Recent_Month                Swanson\n",
      "1      1    Recent_Month               Tostitos\n",
      "2      2    Recent_Month           Kettle Brand\n",
      "3      3    Recent_Month  Cracker Barrel Cheese\n",
      "4      3    Recent_Month                 Jell-O\n",
      "5      4    Recent_Month                Cheetos\n",
      "6      4    Recent_Month        Diet Chris Cola\n",
      "7      4    Recent_Month        Pepperidge Farm\n",
      "8      4    Recent_Month                  Prego\n",
      "9      4    Recent_Month                 Quaker\n",
      "10     4    Recent_Month                     V8\n",
      "11     1  Previous_Month            Grey Poupon\n",
      "12     1  Previous_Month                Swanson\n",
      "13     1  Previous_Month               Tostitos\n",
      "Question 3.\n",
      "Average spend by receipt status:\n",
      "     status  average_spend  receipt_count\n",
      "0  FINISHED          59.83            410\n",
      "1  REJECTED          27.51             57 \n",
      "\n",
      "Higher average spend: FINISHED\n",
      "\n",
      "Question 4.\n",
      "Total items purchased by receipt status:\n",
      "     status  total_items\n",
      "0  FINISHED         5058\n",
      "1  REJECTED          117 \n",
      "\n",
      "Higher total items: FINISHED\n",
      "\n",
      "Question 5.\n",
      "Brand with the most spend (new users, last 6 months):\n",
      "  brand_name  total_spend\n",
      "0   Tostitos     15799.37 \n",
      "\n",
      "Question 6.\n",
      "Brand with the most transactions (new users, last 6 months):\n",
      "  brand_name  transaction_count\n",
      "0    Swanson                 11\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "\n",
    "#Problem 1\n",
    "top5_recent = pd.read_sql_query(\"\"\"\n",
    "WITH brand_receipts AS (\n",
    "  SELECT\n",
    "      strftime('%Y-%m', dates.calendar_date)              AS year_month,\n",
    "      brands.id                                           AS brand_id,\n",
    "      brands.name                                         AS brand_name,\n",
    "      COUNT(DISTINCT receipts.id)                         AS receipt_count\n",
    "  FROM receipts\n",
    "  JOIN dates          ON dates.id        = receipts.purchase_date_id\n",
    "  JOIN receipt_items  ON receipt_items.receipt_id = receipts.id\n",
    "  JOIN brands         ON brands.barcode = receipt_items.barcode\n",
    "  GROUP BY year_month, brands.id\n",
    "),\n",
    "latest_month AS (\n",
    "  SELECT MAX(year_month) AS year_month FROM brand_receipts\n",
    ")\n",
    "SELECT brand_name, receipt_count\n",
    "FROM   brand_receipts\n",
    "WHERE  year_month = (SELECT year_month FROM latest_month)\n",
    "ORDER  BY receipt_count DESC\n",
    "LIMIT  5;\n",
    "\"\"\", conn)\n",
    "print(\"Question 1.\")\n",
    "print(\"Top 5 Brands from the most recent month\")\n",
    "print(top5_recent)\n",
    "# Problem 2 , this one I wasn't entirely sure the best way to display this. It's awkward but it fulfills the requirement\n",
    "rank_compare = pd.read_sql_query(\"\"\"\n",
    "WITH brand_receipts AS (\n",
    "    SELECT\n",
    "        strftime('%Y-%m', dates.calendar_date) AS year_month,\n",
    "        brands.name                            AS brand_name,\n",
    "        COUNT(DISTINCT receipts.id)            AS receipt_count\n",
    "    FROM receipts\n",
    "    JOIN dates         ON dates.id       = receipts.purchase_date_id\n",
    "    JOIN receipt_items ON receipt_items.receipt_id = receipts.id\n",
    "    JOIN brands        ON brands.barcode = receipt_items.barcode\n",
    "    GROUP BY year_month, brands.name\n",
    "),\n",
    "two_latest_months AS (\n",
    "    SELECT year_month\n",
    "    FROM   brand_receipts\n",
    "    GROUP  BY year_month\n",
    "    ORDER  BY year_month DESC\n",
    "    LIMIT  2\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT\n",
    "        year_month,\n",
    "        brand_name,\n",
    "        DENSE_RANK() OVER (PARTITION BY year_month\n",
    "                           ORDER BY receipt_count DESC) AS rank_position\n",
    "    FROM brand_receipts\n",
    "    WHERE year_month IN (SELECT year_month FROM two_latest_months)\n",
    ")\n",
    "\n",
    "-- ▸ wrap the UNION in an outer SELECT so we can order by an expression\n",
    "SELECT *\n",
    "FROM (\n",
    "    -- recent month (top‑5 dense ranks)\n",
    "    SELECT\n",
    "        rank_position     AS Rank,\n",
    "        'Recent_Month'    AS MonthType,\n",
    "        brand_name        AS Brand\n",
    "    FROM ranked\n",
    "    WHERE year_month   = (SELECT MAX(year_month) FROM two_latest_months)\n",
    "      AND rank_position <= 5\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- previous month (top‑5 dense ranks)\n",
    "    SELECT\n",
    "        rank_position,\n",
    "        'Previous_Month',\n",
    "        brand_name\n",
    "    FROM ranked\n",
    "    WHERE year_month   = (SELECT MIN(year_month) FROM two_latest_months)\n",
    "      AND rank_position <= 5\n",
    ") AS combined\n",
    "ORDER BY\n",
    "    CASE MonthType WHEN 'Recent_Month' THEN 0 ELSE 1 END,  -- recent first\n",
    "    Rank,\n",
    "    Brand;\n",
    "\n",
    "\n",
    "\"\"\", conn)\n",
    "print(\"Question 2.\")\n",
    "print(\"Top 5 Brands most recent month vs. previous month\")\n",
    "print(rank_compare)\n",
    "#Problem 3.\n",
    "avg_spend = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    receipt_statuses.status,\n",
    "    ROUND(AVG(receipts.total_spent), 2) AS average_spend,\n",
    "    COUNT(*)                            AS receipt_count\n",
    "FROM receipts\n",
    "JOIN receipt_statuses ON receipt_statuses.id = receipts.receipt_status_id\n",
    "WHERE receipt_statuses.status IN ('FINISHED', 'REJECTED')\n",
    "GROUP BY receipt_statuses.status;\n",
    "\"\"\", conn)\n",
    "print(\"Question 3.\")\n",
    "print(\"Average spend by receipt status:\")\n",
    "print(avg_spend, \"\\n\")\n",
    "higher_avg_status = avg_spend.loc[avg_spend.average_spend.idxmax(), \"status\"]\n",
    "print(f\"Higher average spend: {higher_avg_status}\\n\")\n",
    "\n",
    "# Question 4. \n",
    "total_items = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    receipt_statuses.status,\n",
    "    SUM(receipts.purchased_item_count) AS total_items\n",
    "FROM receipts\n",
    "JOIN receipt_statuses ON receipt_statuses.id = receipts.receipt_status_id\n",
    "WHERE receipt_statuses.status IN ('FINISHED', 'REJECTED')\n",
    "GROUP BY receipt_statuses.status;\n",
    "\"\"\", conn)\n",
    "print(\"Question 4.\")\n",
    "print(\"Total items purchased by receipt status:\")\n",
    "print(total_items, \"\\n\")\n",
    "higher_items_status = total_items.loc[total_items.total_items.idxmax(), \"status\"]\n",
    "print(f\"Higher total items: {higher_items_status}\\n\")\n",
    "\n",
    "# Question 5. The question asked users in the last 6 months but the most recent user is 2021 so going off the 6 months prior to the max\n",
    "brand_most_spend = pd.read_sql_query(\"\"\"\n",
    "WITH new_users AS (\n",
    "  SELECT users.id\n",
    "  FROM   users\n",
    "  JOIN   dates on dates.id = users.created_date_id\n",
    "  WHERE  dates.calendar_date >= date((SELECT MAX(DATE(createdDate)) from users), '-6 months')\n",
    "),\n",
    "receipts_new_users AS (\n",
    "  SELECT id, total_spent\n",
    "  FROM   receipts\n",
    "  WHERE  user_id IN (SELECT id FROM new_users)\n",
    "),\n",
    "brand_spend AS (\n",
    "  SELECT\n",
    "      brands.name                     AS brand_name,\n",
    "      SUM(receipts_new_users.total_spent) AS total_spend\n",
    "  FROM receipts_new_users\n",
    "  JOIN receipt_items ON receipt_items.receipt_id = receipts_new_users.id\n",
    "  JOIN brands        ON brands.barcode         = receipt_items.barcode\n",
    "  GROUP BY brand_name\n",
    ")\n",
    "SELECT brand_name, total_spend\n",
    "FROM   brand_spend\n",
    "ORDER  BY total_spend DESC\n",
    "LIMIT  1;\n",
    "\"\"\", conn)\n",
    "print(\"Question 5.\")\n",
    "print(\"Brand with the most spend (new users, last 6 months):\")\n",
    "print(brand_most_spend, \"\\n\")\n",
    "\n",
    "#Question 6.Again going off the max user date now real life 6 months\n",
    "brand_most_tx = pd.read_sql_query(\"\"\"\n",
    "WITH new_users AS (\n",
    "  SELECT id\n",
    "  FROM   users\n",
    "  WHERE  createdDate >= date((SELECT MAX(DATE(createdDate)) from users), '-6 months')\n",
    "),\n",
    "receipts_new_users AS (\n",
    "  SELECT id\n",
    "  FROM   receipts\n",
    "  WHERE  user_id IN (SELECT id FROM new_users)\n",
    "),\n",
    "brand_transactions AS (\n",
    "  SELECT\n",
    "      brands.name                    AS brand_name,\n",
    "      COUNT(DISTINCT receipt_items.receipt_id) AS transaction_count\n",
    "  FROM receipts_new_users\n",
    "  JOIN receipt_items ON receipt_items.receipt_id = receipts_new_users.id\n",
    "  JOIN brands        ON brands.barcode          = receipt_items.barcode\n",
    "  GROUP BY brand_name\n",
    ")\n",
    "SELECT brand_name, transaction_count\n",
    "FROM   brand_transactions\n",
    "ORDER  BY transaction_count DESC\n",
    "LIMIT  1;\n",
    "\"\"\", conn)\n",
    "print(\"Question 6.\")\n",
    "print(\"Brand with the most transactions (new users, last 6 months):\")\n",
    "print(brand_most_tx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9881923-f6d3-4d3b-9765-8f02bc61b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted: ['brands.json', 'receipts.json', 'users.json']\n"
     ]
    }
   ],
   "source": [
    "#Dataset EDA Third: Evaluate Data Quality Issues in the Data Provided. Ill start with extracted but non-normalized data \n",
    "#I first would look at structural issues with the files\n",
    "import tarfile\n",
    "import pandas\n",
    "gz_folder = 'source_files'\n",
    "unzipped_data = {}\n",
    "\n",
    "# Loop through all .gz files in the folder and unzip them into a dictionary that holds each respective file \n",
    "for filename in os.listdir(gz_folder):\n",
    "    #Constructing the path to open the file\n",
    "    gz_path = os.path.join(gz_folder, filename)\n",
    "    #Opening the file with rt to convert automatically to text \n",
    "    try:\n",
    "        with tarfile.open(gz_path, 'r:gz') as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile():\n",
    "                    extracted = tar.extractfile(member)\n",
    "                    if extracted:\n",
    "                        content = extracted.read().decode('utf-8')\n",
    "                        lines = content.strip().split('\\n')\n",
    "                        pretty_lines = []\n",
    "                        for line in lines:\n",
    "                            if not line.strip():\n",
    "                                continue\n",
    "                            try:\n",
    "                                json_obj = json.loads(line)\n",
    "                                pretty_lines.append(json.dumps(json_obj, indent=4))\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Skipping line in {member.name}: {e}\")\n",
    "                        unzipped_data[member.name] = '\\n\\n'.join(pretty_lines)\n",
    "    except tarfile.ReadError:\n",
    "        with gzip.open(gz_path, 'rt') as file:\n",
    "            base_name = filename[:-3]\n",
    "# In case TAR unzip doesn't work\n",
    "            unzipped_data[base_name] = file.read()\n",
    "\n",
    "print(\"Files extracted:\", list(unzipped_data.keys()))\n",
    "# print(unzipped_data['brands.json'])\n",
    "# print(unzipped_data['users.json'])\n",
    "# print(unzipped_data['receipts.json'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b351847-9857-4a5d-9ed7-2f0bed26db7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating brands.json...\n",
      "brands.json is valid NDJSON.\n",
      "\n",
      "Validating receipts.json...\n",
      "receipts.json is valid NDJSON.\n",
      "\n",
      "Validating users.json...\n",
      "Invalid JSON in users.json at line 1: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "     Line content: {\n"
     ]
    }
   ],
   "source": [
    "#Next checking if all files are valid NDJSON. users.json seems to have some issue with non-printing characters\n",
    "import json\n",
    "\n",
    "for file_name, content in unzipped_data.items():\n",
    "    print(f\"Validating {file_name}...\")\n",
    "    lines = content.strip().split('\\n')\n",
    "    for idx, line in enumerate(lines, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Invalid JSON in {file_name} at line {idx}: {e}\")\n",
    "            print(f\"     Line content: {line[:200]}{'...' if len(line) > 200 else ''}\")\n",
    "            break  # Optionally stop at the first error per file\n",
    "    else:\n",
    "        print(f\"{file_name} is valid NDJSON.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c2351514-c1c6-4d44-942c-9bbef3b3e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the pretty print issue\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Explicitly define which files use NDJSON structure\n",
    "ndjson_files = {'receipts.json', 'brands.json'}\n",
    "pretty_json_files = {'users.json'}\n",
    "\n",
    "for filename, content in unzipped_data.items():\n",
    "    try:\n",
    "        stripped = content.strip()\n",
    "\n",
    "        if filename in ndjson_files:\n",
    "            # NDJSON: each line is a standalone JSON object\n",
    "            json_objects = [\n",
    "                json.loads(line)\n",
    "                for line in stripped.splitlines()\n",
    "                if line.strip()\n",
    "            ]\n",
    "        elif filename in pretty_json_files:\n",
    "            # Pretty-printed or compact JSON with blank-line separation\n",
    "            blocks = stripped.split('\\n\\n')\n",
    "            json_objects = [\n",
    "                json.loads(block)\n",
    "                for block in blocks\n",
    "                if block.strip()\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format for file: {filename}\")\n",
    "\n",
    "        df = pd.DataFrame(json_objects)\n",
    "        cleaned_dataframes[filename] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {filename} into DataFrame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c80a183-a231-4975-89a0-554df7d63754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users json:                                      _id  active               createdDate  \\\n",
      "0  {'$oid': '5ff1e194b6a9d73a3a9f1052'}    True  {'$date': 1609687444800}   \n",
      "1  {'$oid': '5ff1e194b6a9d73a3a9f1052'}    True  {'$date': 1609687444800}   \n",
      "2  {'$oid': '5ff1e194b6a9d73a3a9f1052'}    True  {'$date': 1609687444800}   \n",
      "3  {'$oid': '5ff1e1eacfcf6c399c274ae6'}    True  {'$date': 1609687530554}   \n",
      "4  {'$oid': '5ff1e194b6a9d73a3a9f1052'}    True  {'$date': 1609687444800}   \n",
      "\n",
      "                  lastLogin      role signUpSource state  \n",
      "0  {'$date': 1609687537858}  consumer        Email    WI  \n",
      "1  {'$date': 1609687537858}  consumer        Email    WI  \n",
      "2  {'$date': 1609687537858}  consumer        Email    WI  \n",
      "3  {'$date': 1609687530597}  consumer        Email    WI  \n",
      "4  {'$date': 1609687537858}  consumer        Email    WI  \n",
      "Brands json:                                      _id       barcode        category  \\\n",
      "0  {'$oid': '601ac115be37ce2ead437551'}  511111019862          Baking   \n",
      "1  {'$oid': '601c5460be37ce2ead43755f'}  511111519928       Beverages   \n",
      "2  {'$oid': '601ac142be37ce2ead43755d'}  511111819905          Baking   \n",
      "3  {'$oid': '601ac142be37ce2ead43755a'}  511111519874          Baking   \n",
      "4  {'$oid': '601ac142be37ce2ead43755e'}  511111319917  Candy & Sweets   \n",
      "\n",
      "       categoryCode                                                cpg  \\\n",
      "0            BAKING  {'$id': {'$oid': '601ac114be37ce2ead437550'}, ...   \n",
      "1         BEVERAGES  {'$id': {'$oid': '5332f5fbe4b03c9a25efd0ba'}, ...   \n",
      "2            BAKING  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "3            BAKING  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "4  CANDY_AND_SWEETS  {'$id': {'$oid': '5332fa12e4b03c9a25efd1e7'}, ...   \n",
      "\n",
      "                        name topBrand                      brandCode  \n",
      "0  test brand @1612366101024    False                            NaN  \n",
      "1                  Starbucks    False                      STARBUCKS  \n",
      "2  test brand @1612366146176    False  TEST BRANDCODE @1612366146176  \n",
      "3  test brand @1612366146051    False  TEST BRANDCODE @1612366146051  \n",
      "4  test brand @1612366146827    False  TEST BRANDCODE @1612366146827  \n",
      "0    {'$id': {'$oid': '601ac114be37ce2ead437550'}, ...\n",
      "1    {'$id': {'$oid': '5332f5fbe4b03c9a25efd0ba'}, ...\n",
      "2    {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...\n",
      "3    {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...\n",
      "4    {'$id': {'$oid': '5332fa12e4b03c9a25efd1e7'}, ...\n",
      "Name: cpg, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Checking for nested objects\n",
    "print(\"Users json: \",cleaned_dataframes['users.json'].head())\n",
    "print(\"Brands json: \",cleaned_dataframes['brands.json'].head())\n",
    "print(cleaned_dataframes['brands.json']['cpg'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2d19439-8f20-42f0-b3a2-77070159936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users json:                          _id  active             createdDate  \\\n",
      "0  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "1  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "2  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "3  5ff1e1eacfcf6c399c274ae6    True 2021-01-03 09:25:30.554   \n",
      "4  5ff1e194b6a9d73a3a9f1052    True 2021-01-03 09:24:04.800   \n",
      "\n",
      "                lastLogin      role signUpSource state  \n",
      "0 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "1 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "2 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "3 2021-01-03 09:25:30.597  consumer        Email    WI  \n",
      "4 2021-01-03 09:25:37.858  consumer        Email    WI  \n",
      "Brands json:                          _id       barcode        category      categoryCode  \\\n",
      "0  601ac115be37ce2ead437551  511111019862          Baking            BAKING   \n",
      "1  601c5460be37ce2ead43755f  511111519928       Beverages         BEVERAGES   \n",
      "2  601ac142be37ce2ead43755d  511111819905          Baking            BAKING   \n",
      "3  601ac142be37ce2ead43755a  511111519874          Baking            BAKING   \n",
      "4  601ac142be37ce2ead43755e  511111319917  Candy & Sweets  CANDY_AND_SWEETS   \n",
      "\n",
      "                                                 cpg  \\\n",
      "0  {'$id': {'$oid': '601ac114be37ce2ead437550'}, ...   \n",
      "1  {'$id': {'$oid': '5332f5fbe4b03c9a25efd0ba'}, ...   \n",
      "2  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "3  {'$id': {'$oid': '601ac142be37ce2ead437559'}, ...   \n",
      "4  {'$id': {'$oid': '5332fa12e4b03c9a25efd1e7'}, ...   \n",
      "\n",
      "                        name topBrand                      brandCode  \n",
      "0  test brand @1612366101024    False                            NaN  \n",
      "1                  Starbucks    False                      STARBUCKS  \n",
      "2  test brand @1612366146176    False  TEST BRANDCODE @1612366146176  \n",
      "3  test brand @1612366146051    False  TEST BRANDCODE @1612366146051  \n",
      "4  test brand @1612366146827    False  TEST BRANDCODE @1612366146827  \n",
      "Receipts json:                          _id  bonusPointsEarned  \\\n",
      "0  5ff1e1eb0a720f0523000575              500.0   \n",
      "1  5ff1e1bb0a720f052300056b              150.0   \n",
      "2  5ff1e1f10a720f052300057a                5.0   \n",
      "3  5ff1e1ee0a7214ada100056f                5.0   \n",
      "4  5ff1e1d20a7214ada1000561                5.0   \n",
      "\n",
      "                             bonusPointsEarnedReason          createDate  \\\n",
      "0  Receipt number 2 completed, bonus point schedu... 2021-01-03 09:25:31   \n",
      "1  Receipt number 5 completed, bonus point schedu... 2021-01-03 09:24:43   \n",
      "2                         All-receipts receipt bonus 2021-01-03 09:25:37   \n",
      "3                         All-receipts receipt bonus 2021-01-03 09:25:34   \n",
      "4                         All-receipts receipt bonus 2021-01-03 09:25:06   \n",
      "\n",
      "          dateScanned        finishedDate          modifyDate  \\\n",
      "0 2021-01-03 09:25:31 2021-01-03 09:25:31 2021-01-03 09:25:36   \n",
      "1 2021-01-03 09:24:43 2021-01-03 09:24:43 2021-01-03 09:24:48   \n",
      "2 2021-01-03 09:25:37                 NaT 2021-01-03 09:25:42   \n",
      "3 2021-01-03 09:25:34 2021-01-03 09:25:34 2021-01-03 09:25:39   \n",
      "4 2021-01-03 09:25:06 2021-01-03 09:25:11 2021-01-03 09:25:11   \n",
      "\n",
      "    pointsAwardedDate pointsEarned        purchaseDate  purchasedItemCount  \\\n",
      "0 2021-01-03 09:25:31        500.0 2021-01-02 18:00:00                 5.0   \n",
      "1 2021-01-03 09:24:43        150.0 2021-01-02 09:24:43                 2.0   \n",
      "2                 NaT            5 2021-01-02 18:00:00                 1.0   \n",
      "3 2021-01-03 09:25:34          5.0 2021-01-02 18:00:00                 4.0   \n",
      "4 2021-01-03 09:25:06          5.0 2021-01-02 09:25:06                 2.0   \n",
      "\n",
      "                              rewardsReceiptItemList rewardsReceiptStatus  \\\n",
      "0  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "1  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "2  [{'needsFetchReview': False, 'partnerItemId': ...             REJECTED   \n",
      "3  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "4  [{'barcode': '4011', 'description': 'ITEM NOT ...             FINISHED   \n",
      "\n",
      "  totalSpent                    userId  \n",
      "0      26.00  5ff1e1eacfcf6c399c274ae6  \n",
      "1      11.00  5ff1e194b6a9d73a3a9f1052  \n",
      "2      10.00  5ff1e1f1cfcf6c399c274b0b  \n",
      "3      28.00  5ff1e1eacfcf6c399c274ae6  \n",
      "4       1.00  5ff1e194b6a9d73a3a9f1052  \n"
     ]
    }
   ],
   "source": [
    "#Cleaning up nested objects. Looking at the data I can see cpg has ref and id columns I would flatten those (but won't for the data validation excersize.\n",
    "# In general I wouldn't want to have nested objects if it's not required (i.e nested objects with single members)\n",
    "from datetime import datetime\n",
    "#Cleaning up nested objects\n",
    "\n",
    "for df in cleaned_dataframes.values():\n",
    "    # 1. Normalize _id.$oid to a string\n",
    "    if '_id' in df.columns:\n",
    "        df['_id'] = df['_id'].apply(\n",
    "            lambda x: x.get('$oid') if isinstance(x, dict) and '$oid' in x else x\n",
    "        )\n",
    "\n",
    "    # 2. Normalize all columns that include \"date\" (case-insensitive)\n",
    "    for col in df.columns:\n",
    "        if 'date' or 'lastlogin' in col.lower():\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: datetime.fromtimestamp(x['$date'] / 1000.0)\n",
    "                if isinstance(x, dict) and '$date' in x else x\n",
    "            )\n",
    "print(\"Users json: \",cleaned_dataframes['users.json'].head())\n",
    "print(\"Brands json: \",cleaned_dataframes['brands.json'].head())\n",
    "print(\"Receipts json: \",cleaned_dataframes['receipts.json'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55393ae5-ec9a-4f70-8302-469b6933930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Enforce foreign keys between users, receipts, brands and receipt items\\n2. Check for missing dates (such as a missing pointsAwardedDate but filled in points earned\\n3. Check for amounts, quantities that deviate significantly from the mean\\n4. Check for non valid states.\\n5. Check types (for example a non numeric barcode etc.)\\n6. Check duplicate receipts (i.e a user purchasing the exact same thing twice on the same date)\\n7. Look for things like  test brand @1612366146827 and ensure they are valid for the use case\\n8. Check that a purchase date is older than the user that bought it\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not sure it's necessary to fully write out all tests after this. In general here's a non-exhaustive list of things I would check\n",
    "# after JSON structural tests.\n",
    "\"\"\"\n",
    "1. Enforce foreign keys between users, receipts, brands and receipt items\n",
    "2. Check for missing dates (such as a missing pointsAwardedDate but filled in points earned\n",
    "3. Check for amounts, quantities that deviate significantly from the mean\n",
    "4. Check for non valid states.\n",
    "5. Check types (for example a non numeric barcode etc.)\n",
    "6. Check duplicate receipts (i.e a user purchasing the exact same thing twice on the same date)\n",
    "7. Look for things like  test brand @1612366146827 and ensure they are valid for the use case\n",
    "8. Check that a purchase date is older than the user that bought it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fafb8434-df37-470c-905e-4f0a7d58de4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (4237455029.py, line 35)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Subject: Data Alignment and Open Questions\n",
    "\n",
    "Hi [Stakeholder's Name],\n",
    "\n",
    "I hope you're doing well.\n",
    "\n",
    "As we begin working with the dataset, I want to make sure we're aligned on expectations and priorities. Below are several key questions and observations that will help guide next steps and ensure the data is optimized for your use cases.\n",
    "\n",
    "Questions about the data:\n",
    "\n",
    "What are “partner items” and how should they be interpreted in analysis?\n",
    "\n",
    "The current “brands” feed appears to list individual items rather than brand groupings. Should there be a separate feed for brands?\n",
    "\n",
    "We’ve identified what appears to be test data (e.g., users with the role \"fetch-staff\"). Is this data intended for production use? Are there any valid purchases from non-consumer roles?\n",
    "\n",
    "What are the expected use cases for this data (e.g., reporting, data science, real-time applications)?\n",
    "\n",
    "What is the anticipated data volume on a daily, monthly, and annual basis?\n",
    "\n",
    "Will the data be incrementally loaded or fully refreshed?\n",
    "\n",
    "How frequently will the reports or dashboards built on this data be accessed?\n",
    "\n",
    "How many users will be interacting with the final outputs?\n",
    "\n",
    "What are the expectations around data freshness?\n",
    "\n",
    "Are there any known pain points we should be aware of (e.g., slow report refreshes, volume-related issues)?\n",
    "\n",
    "Initial data quality issues:\n",
    "\n",
    "We encountered issues with the user dataset due to formatting inconsistencies. Some fields were missing or empty, which interfered with loading. We'll need to assess the scale of these issues and determine which fields are critical versus optional for the target workloads.\n",
    "\n",
    "To resolve the quality issues, we’ll need:\n",
    "\n",
    "Information about the source system\n",
    "\n",
    "Access to the original data source, if possible\n",
    "\n",
    "Clarification on which fields are essential for the end use cases\n",
    "\n",
    "Additional considerations for performance and scalability:\n",
    "\n",
    "Depending on data volume and query performance, we may need to consider strategies like denormalization or alternate modeling approaches\n",
    "\n",
    "The ETL tools and database platform in use will influence how we structure and optimize the data\n",
    "\n",
    "For larger or more complex workloads, we may need to implement partitioning or evaluate distributed computing strategies\n",
    "\n",
    "It’s also important to understand team capabilities so we can design solutions that align with existing skill sets\n",
    "\n",
    "Please let me know your thoughts on the above, or if you'd prefer to set up a quick meeting to walk through the details.\n",
    "\n",
    "Best regards,\n",
    "Harrison Reid\n",
    "Senior Data Engineer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d29173-07b6-41d7-a5a1-bb205219eabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
